{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改预训练模型的缓存目录\n",
    "import os\n",
    "\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = r\"G:\\code\\pretrain_model_dir\\_modelscope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 20:03:11,449 - modelscope - INFO - PyTorch version 2.1.0+cu118 Found.\n",
      "2023-10-21 20:03:11,452 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-10-21 20:03:11,453 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-10-21 20:03:11,561 - modelscope - INFO - Loading done! Current index file version is 1.9.3, with md5 dcadab61123e85c4b246f778e50d74cc and a total number of 943 components indexed\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-21 20:03:14,565 - modelscope - INFO - Use user-specified model revision: v1.0.5\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "model_dir = snapshot_download('qwen/Qwen-14B-Chat-Int4', revision='v1.0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "CUDA extension not installed.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金子浮在水面上是因为它的密度比水小。密度是指物质的质量与其体积的比值，金子的密度约为19.32克/立方厘米，而水的密度约为1克/立方厘米。因此，当金子被放入水中时，它的质量比水大，但体积比水小，所以它会浮在水面上。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"金子为什么浮在水面上\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method chat in module transformers_modules.Qwen-14B-Chat-Int4.modeling_qwen:\n",
      "\n",
      "chat(tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, query: str, history: Union[List[Tuple[str, str]], NoneType], system: str = 'You are a helpful assistant.', append_history: bool = True, stream: Union[bool, NoneType] = <object object at 0x000001F781D48DB0>, stop_words_ids: Union[List[List[int]], NoneType] = None, generation_config: Union[transformers.generation.configuration_utils.GenerationConfig, NoneType] = None, **kwargs) -> Tuple[str, List[Tuple[str, str]]] method of transformers_modules.Qwen-14B-Chat-Int4.modeling_qwen.QWenLMHeadModel instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# int4 的好慢. 试试原始的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 20:47:42,492 - modelscope - INFO - PyTorch version 2.1.0+cu118 Found.\n",
      "2023-10-21 20:47:42,495 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-10-21 20:47:42,495 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-10-21 20:47:42,620 - modelscope - INFO - Loading done! Current index file version is 1.9.3, with md5 dcadab61123e85c4b246f778e50d74cc and a total number of 943 components indexed\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-21 20:47:45,408 - modelscope - INFO - Use user-specified model revision: v1.0.5\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "model_dir = snapshot_download('qwen/Qwen-14B-Chat', revision='v1.0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:13<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    False,\n",
    "    True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float16, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype, model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是因为金子的密度小于水，所以它会浮在水面上。密度是物体质量和体积的比值，而水的密度大约为1克/立方厘米，金子的密度约为19.3克/立方厘米。由于金子的密度小于水，所以相同质量的金子和水相比，金子的体积更大，因此金子会漂浮在水面上。\n",
      "需要注意的是，虽然金子浮在水面上，但是它的密度仍然大于许多其他物质，如铜、铁等，这些物质通常会在水中下沉。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"为什么金子浮在水面上\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
