{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改预训练模型的缓存目录\n",
    "import os\n",
    "\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = r\"G:\\code\\pretrain_model_dir\\_modelscope\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://modelscope.cn/topic/9ae88b6a1ffd4de59a9f1948314ebc2b/pub/summary\n",
    "\n",
    "# 我需要什么?\n",
    "\n",
    "我想要一个摘要和关键字抽取的功能, 用于从 app 描述信息里抽取功能描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = \"\"\"\n",
    "抖音是一个帮助用户表达自我，记录美好生活的短视频平台。\n",
    "\n",
    "● 记录美好在抖音\n",
    "智能匹配音乐、一键卡点视频，还有超多原创特殊效果、滤镜、场景切换帮你一秒变大片，让你的生活轻松记录在抖音！\n",
    "\n",
    "● 实用内容在抖音\n",
    "生活妙招、美食做法、旅行攻略、科技知识、新闻时事、同城资讯，你需要的实用内容都在抖音！\n",
    "\n",
    "● 各行各业在抖音\n",
    "原创音乐人、京剧演员、中国科学院教授、非遗传承人、烧烤摊老板、快递小哥等，每个人真实的生活都在抖音！\n",
    "\n",
    "全民记录自我，生活的美好都在这里！\n",
    "\n",
    "＠联系我们\n",
    "-应用内反馈:「我」-「右上角菜单」-「设置」-「反馈与帮助」\n",
    "-官方邮箱：feedback@douyin.com\n",
    "-客户服务热线：95152\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 SiameseUniNLU零样本通用自然语言理解-中文-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:23:25,527 - modelscope - INFO - PyTorch version 2.0.0+cu118 Found.\n",
      "2023-08-17 22:23:25,529 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-08-17 22:23:25,530 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-08-17 22:23:25,530 - modelscope - INFO - No valid ast index found from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer, generating ast index from prebuilt!\n",
      "2023-08-17 22:23:25,652 - modelscope - INFO - Loading done! Current index file version is 1.8.3, with md5 f01211d2b2e03771df6500e25770738b and a total number of 895 components indexed\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-17 22:23:27,492 - modelscope - INFO - Use user-specified model revision: v1.0\n",
      "Downloading: 100%|██████████| 554/554 [00:00<00:00, 111kB/s]\n",
      "Downloading: 100%|██████████| 379/379 [00:00<00:00, 95.0kB/s]\n",
      "Downloading: 100%|██████████| 93.0k/93.0k [00:00<00:00, 1.13MB/s]\n",
      "Downloading: 100%|██████████| 390M/390M [00:14<00:00, 28.6MB/s] \n",
      "Downloading: 100%|██████████| 9.79k/9.79k [00:00<00:00, 2.00MB/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 749kB/s]\n",
      "2023-08-17 22:23:45,738 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uninlu_chinese-base\n",
      "2023-08-17 22:23:45,739 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uninlu_chinese-base.\n",
      "2023-08-17 22:23:45,741 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uninlu_chinese-base\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "semantic_cls = pipeline(Tasks.siamese_uie, 'damo/nlp_structbert_siamese-uninlu_chinese-base', model_revision='v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_cls.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '人物', 'span': '谷口清太郎', 'offset': [18, 23]}],\n",
       "  [{'type': '组织机构', 'span': '北大', 'offset': [8, 10]}],\n",
       "  [{'type': '组织机构', 'span': '名古屋铁道', 'offset': [11, 16]}]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 命名实体识别 {实体类型: None}\n",
    "semantic_cls(\n",
    "    input='1944年毕业于北大的名古屋铁道会长谷口清太郎等人在日本积极筹资，共筹款2.7亿日元，参加捐款的日本企业有69家。', \n",
    "    schema={\n",
    "        '人物': None,\n",
    "        '地理位置': None,\n",
    "        '组织机构': None\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '人物', 'span': '谷爱凌', 'offset': [32, 35]},\n",
       "   {'type': '比赛项目(赛事名称)', 'span': '滑雪女子大跳台决赛', 'offset': [18, 27]}],\n",
       "  [{'type': '人物', 'span': '谷爱凌', 'offset': [32, 35]},\n",
       "   {'type': '比赛项目(赛事名称)', 'span': '滑雪男子大跳台决赛', 'offset': [55, 64]}],\n",
       "  [{'type': '人物', 'span': '谷爱凌', 'offset': [32, 35]},\n",
       "   {'type': '选手国籍(国籍)', 'span': '中国', 'offset': [28, 30]}],\n",
       "  [{'type': '人物', 'span': '小泉次郎', 'offset': [69, 73]},\n",
       "   {'type': '比赛项目(赛事名称)', 'span': '滑雪女子大跳台决赛', 'offset': [18, 27]}],\n",
       "  [{'type': '人物', 'span': '小泉次郎', 'offset': [69, 73]},\n",
       "   {'type': '比赛项目(赛事名称)', 'span': '滑雪男子大跳台决赛', 'offset': [55, 64]}],\n",
       "  [{'type': '人物', 'span': '小泉次郎', 'offset': [69, 73]},\n",
       "   {'type': '选手国籍(国籍)', 'span': '中国', 'offset': [28, 30]}]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 关系抽取 {主语实体类型: {关系(宾语实体类型): None}}\n",
    "semantic_cls(\n",
    "\tinput='在北京冬奥会自由式中，2月8日上午，滑雪女子大跳台决赛中中国选手谷爱凌以188.25分获得金牌。2月9日上午，滑雪男子大跳台决赛中日本选手小泉次郎以188.25分获得银牌！', \n",
    "  \tschema={\n",
    "        '人物': {\n",
    "            '比赛项目(赛事名称)': None,\n",
    "            '参赛地点(城市)': None,\n",
    "            '获奖时间(时间)': None,\n",
    "            '选手国籍(国籍)': None\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '胜负(事件触发词)', 'span': '负于', 'offset': [19, 21]},\n",
       "   {'type': '时间', 'span': '7月28日', 'offset': [0, 5]}],\n",
       "  [{'type': '胜负(事件触发词)', 'span': '负于', 'offset': [19, 21]},\n",
       "   {'type': '败者', 'span': '天津泰达', 'offset': [6, 10]}],\n",
       "  [{'type': '胜负(事件触发词)', 'span': '负于', 'offset': [19, 21]},\n",
       "   {'type': '胜者', 'span': '天津泰达', 'offset': [6, 10]}],\n",
       "  [{'type': '胜负(事件触发词)', 'span': '负于', 'offset': [19, 21]},\n",
       "   {'type': '胜者', 'span': '天津天海', 'offset': [21, 25]}]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事件抽取 {事件类型（事件触发词）: {参数类型: None}}\n",
    "semantic_cls(\n",
    "\tinput='7月28日，天津泰达在德比战中以0-1负于天津天海。', \n",
    "  \tschema={\n",
    "        '胜负(事件触发词)': {\n",
    "            '时间': None,\n",
    "            '败者': None,\n",
    "            '胜者': None,\n",
    "            '赛事名称': None\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '属性词', 'span': '音质', 'offset': [4, 6]},\n",
       "   {'type': '情感词', 'span': '很好', 'offset': [6, 8]}],\n",
       "  [{'type': '属性词', 'span': '音质', 'offset': [4, 6]},\n",
       "   {'type': '情感词', 'span': '快', 'offset': [13, 14]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [9, 13]},\n",
       "   {'type': '情感词', 'span': '快', 'offset': [13, 14]}]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 属性情感抽取 {属性词: {情感词: None}}\n",
    "semantic_cls(\n",
    "\tinput='很满意，音质很好，发货速度快，值得购买', \n",
    "  \tschema={\n",
    "        '属性词': {\n",
    "            '情感词': None,\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '属性词', 'span': '#', 'offset': [0, 1]},\n",
       "   {'type': '情感词', 'span': '很满意', 'offset': [1, 4]}],\n",
       "  [{'type': '属性词', 'span': '#', 'offset': [0, 1]},\n",
       "   {'type': '情感词', 'span': '值得购买', 'offset': [16, 20]}],\n",
       "  [{'type': '属性词', 'span': '音质', 'offset': [5, 7]},\n",
       "   {'type': '情感词', 'span': '很好', 'offset': [7, 9]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [10, 14]},\n",
       "   {'type': '情感词', 'span': '快', 'offset': [14, 15]}]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 允许属性词缺省，#表示缺省\n",
    "semantic_cls(\n",
    "\tinput='#很满意，音质很好，发货速度快，值得购买', \n",
    "  \tschema={\n",
    "        '属性词': {\n",
    "            '情感词': None,\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '属性词', 'span': '音质', 'offset': [4, 6]},\n",
       "   {'type': '正向情感(情感词)', 'span': '很好', 'offset': [6, 8]}],\n",
       "  [{'type': '属性词', 'span': '音质', 'offset': [4, 6]},\n",
       "   {'type': '正向情感(情感词)', 'span': '快', 'offset': [13, 14]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [9, 13]},\n",
       "   {'type': '正向情感(情感词)', 'span': '很满意', 'offset': [0, 3]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [9, 13]},\n",
       "   {'type': '正向情感(情感词)', 'span': '快', 'offset': [13, 14]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [9, 13]},\n",
       "   {'type': '正向情感(情感词)', 'span': '值得购买', 'offset': [15, 19]}]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 支持情感分类\n",
    "semantic_cls(\n",
    "\tinput='很满意，音质很好，发货速度快，值得购买', \n",
    "  \tschema={\n",
    "        '属性词': {\n",
    "            \"正向情感(情感词)\": None, \n",
    "            \"负向情感(情感词)\": None, \n",
    "            \"中性情感(情感词)\": None\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '在下面的描述中，代词“她”指代的是“妹妹”吗？',\n",
       "    'span': '不是',\n",
       "    'offset': [3, 5]}]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指代消解，判断选项通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔\n",
    "semantic_cls(\n",
    "\tinput='是的,不是|哥哥点了点头。“我这几年苦哇……现在玲玲也大一点了，所以……”他望着妹妹（候选词），脸上显出一副要求她(代词)谅解的表情。', \n",
    "  \tschema={\n",
    "        '在下面的描述中，代词“她”指代的是“妹妹”吗？': None\n",
    "        }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '情感分类', 'span': '负向', 'offset': [3, 5]}]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 情感分类，情感标签通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔；同时也支持情绪分类任务，换成相应情绪标签即可，e.g. \"无情绪,积极,愤怒,悲伤,恐惧,惊奇\"\n",
    "semantic_cls(\n",
    "\tinput='正向,负向|有点看不下去了，看作者介绍就觉得挺矫情了，文字也弱了点。后来才发现 大家对这本书评价都很低。亏了。', \n",
    "  \tschema={\n",
    "        '情感分类': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '分类', 'span': '教育', 'offset': [23, 25]}]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本分类，文本标签通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔\n",
    "semantic_cls(\n",
    "\tinput='民生故事,文化,娱乐,体育,财经,房产,汽车,教育,科技,军事,旅游,国际,证券股票,农业三农,电竞游戏|学校召开2018届升学及出国深造毕业生座谈会就业指导', \n",
    "  \tschema={\n",
    "        '分类': None\n",
    "        }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '文本匹配', 'span': '不相似', 'offset': [3, 6]}]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本匹配，文本相似度标签通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔；输入文本由两段文本组成，并用“&”隔开\n",
    "semantic_cls(\n",
    "\tinput='相似,不相似|摄像头区域遮挡屏幕&通话遮挡屏幕黑屏正常', \n",
    "  \tschema={\n",
    "        '文本匹配': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '下面两句话的意思是否相同', 'span': '是的', 'offset': [0, 2]}],\n",
       "  [{'type': '下面两句话的意思是否相同', 'span': '不是', 'offset': [3, 5]}]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本匹配也可以用下面这种方式组织，判断选项通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔；输入文本由两段文本组成，并分别用“句子1”和“句子2”区分\n",
    "semantic_cls(\n",
    "\tinput='是的,不是|句子1：摄像头区域遮挡屏幕；句子2：通话遮挡屏幕黑屏正常', \n",
    "  \tschema={\n",
    "        '下面两句话的意思是否相同': None\n",
    "        }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '段落2和段落1的关系是：', 'span': '矛盾', 'offset': [3, 5]}]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自然语言推理，文本关系标签通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔；输入文本由两段文本组成，并分别用“段落1”和“段落2”区分\n",
    "semantic_cls(\n",
    "\tinput='蕴含,矛盾,中立|段落1：是,但是你比如说像现在这种情况,是不是就是说咱们离它就绝对人类是再也没有任何可能性了；段落2：我对人类可能性有所思考', \n",
    "  \tschema={\n",
    "        '段落2和段落1的关系是：': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': 'B为什么不坐飞机?', 'span': '坐飞机头晕', 'offset': [12, 17]}]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择类阅读理解，选项通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔\n",
    "semantic_cls(\n",
    "\tinput='飞机票太贵,时间来不及,坐飞机头晕,飞机票太便宜|A：最近飞机票打折挺多的，你还是坐飞机去吧。B：反正又不是时间来不及，飞机再便宜我也不坐，我一听坐飞机就头晕。', \n",
    "  \tschema={\n",
    "        'B为什么不坐飞机?': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '大莱龙铁路位于哪里？', 'span': '山东', 'offset': [7, 9]}]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 抽取类阅读理解\n",
    "semantic_cls(\n",
    "\tinput='大莱龙铁路位于山东省北部环渤海地区，西起位于益羊铁路的潍坊大家洼车站，向东经海化、寿光、寒亭、昌邑、平度、莱州、招远、终到龙口，连接山东半岛羊角沟、潍坊、莱州、龙口四个港口，全长175公里，工程建设概算总投资11.42亿元。铁路西与德大铁路、黄大铁路在大家洼站接轨，东与龙烟铁路相连。大莱龙铁路于1997年11月批复立项，2002年12月28日全线铺通，2005年6月建成试运营，是横贯山东省北部的铁路干线德龙烟铁路的重要组成部分，构成山东省北部沿海通道，并成为环渤海铁路网的南部干线。铁路沿线设有大家洼站、寒亭站、昌邑北站、海天站、平度北站、沙河站、莱州站、朱桥站、招远站、龙口西站、龙口北站、龙口港站。大莱龙铁路官方网站', \n",
    "  \tschema={\n",
    "        '大莱龙铁路位于哪里？': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全任务零样本学习-mT5分类增强版-中文-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:32:10,941 - modelscope - INFO - Use user-specified model revision: v1.0.0\n",
      "Downloading: 100%|██████████| 724/724 [00:00<00:00, 241kB/s]\n",
      "Downloading: 100%|██████████| 199/199 [00:00<00:00, 28.4kB/s]\n",
      "Downloading: 100%|██████████| 2.17G/2.17G [01:25<00:00, 27.2MB/s]\n",
      "Downloading: 100%|██████████| 12.7k/12.7k [00:00<00:00, 459kB/s]\n",
      "Downloading: 100%|██████████| 74.0/74.0 [00:00<00:00, 24.7kB/s]\n",
      "Downloading: 100%|██████████| 4.11M/4.11M [00:00<00:00, 10.6MB/s]\n",
      "Downloading: 100%|██████████| 499/499 [00:00<00:00, 99.7kB/s]\n",
      "2023-08-17 22:33:39,801 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_mt5_zero-shot-augment_chinese-base\n",
      "2023-08-17 22:33:39,802 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_mt5_zero-shot-augment_chinese-base.\n",
      "2023-08-17 22:33:39,804 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_mt5_zero-shot-augment_chinese-base\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "2023-08-17 22:33:43,920 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 22:33:43,920 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 22:33:43,921 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_mt5_zero-shot-augment_chinese-base'}. trying to build by task and model information.\n",
      "2023-08-17 22:33:43,926 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 22:33:43,927 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 22:33:43,927 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_mt5_zero-shot-augment_chinese-base', 'first_sequence': 'sentence'}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "\n",
    "t2t_generator = pipeline(\"text2text-generation\", \"damo/nlp_mt5_zero-shot-augment_chinese-base\", model_revision=\"v1.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:35:16,613 - modelscope - WARNING - task text2text-generation input definition is missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '文化'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(t2t_generator(\"文本分类。\\n候选标签：故事,房产,娱乐,文化,游戏,国际,股票,科技,军事,教育。\\n文本内容：他们的故事平静而闪光，一代人奠定沉默的基石，让中国走向繁荣。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '无线Mesh网,路由协议,环境感知推理'}\n"
     ]
    }
   ],
   "source": [
    "print(t2t_generator(\"抽取关键词：\\n在分析无线Mesh网路由协议所面临挑战的基础上,结合无线Mesh网络的性能要求,以优化链路状态路由(OLSR)协议为原型,采用跨层设计理论,提出了一种基于链路状态良好程度的路由协议LR-OLSR.该协议引入了认知无线网络中的环境感知推理思想,通过时节点负载、链路投递率和链路可用性等信息进行感知,并以此为依据对链路质量进行推理,获得网络中源节点和目的节点对之间各路径状态良好程度的评价,将其作为路由选择的依据,实现对路由的优化选择,提高网络的吞吐量,达到负载均衡.通过与OLSR及其典型改进协议P-OLSR、SC-OLSR的对比仿真结果表明,LR-OLSB能够提高网络中分组的递交率,降低平均端到端时延,在一定程度上达到负载均衡.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '短视频平台,科技知识,短视频平台,短视频,短视频,中国'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"抽取关键词：\\n{app}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '短视频平台,生活妙招,科技知识,短视频平台,短视频,'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"提取关键字：\\n{app}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '短视频平台,短视频,生活妙招,科技知识,新闻时事,'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"提取重点：\\n{app}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '短视频平台,短视频,生活妙招,科技知识,新闻时事,'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"提取关键短语：\\n{app}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SiameseUIE通用信息抽取-中文-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:44:09,173 - modelscope - INFO - Use user-specified model revision: v1.0\n",
      "Downloading: 100%|██████████| 554/554 [00:00<00:00, 92.5kB/s]\n",
      "Downloading: 100%|██████████| 379/379 [00:00<00:00, 54.1kB/s]\n",
      "Downloading: 100%|██████████| 55.8k/55.8k [00:00<00:00, 883kB/s]\n",
      "Downloading: 100%|██████████| 32.6k/32.6k [00:00<00:00, 755kB/s]\n",
      "Downloading: 100%|██████████| 390M/390M [00:14<00:00, 28.5MB/s] \n",
      "Downloading: 100%|██████████| 2.24k/2.24k [00:00<00:00, 458kB/s]\n",
      "Downloading: 100%|██████████| 7.24k/7.24k [00:00<00:00, 831kB/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 942kB/s] \n",
      "Downloading: 100%|██████████| 68.2k/68.2k [00:00<00:00, 932kB/s]\n",
      "2023-08-17 22:44:26,196 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uie_chinese-base\n",
      "2023-08-17 22:44:26,196 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uie_chinese-base.\n",
      "2023-08-17 22:44:26,198 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uie_chinese-base\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "semantic_cls = pipeline(Tasks.siamese_uie, 'damo/nlp_structbert_siamese-uie_chinese-base', model_revision='v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '属性词', 'span': '抖音', 'offset': [1, 3]},\n",
       "   {'type': '重点词', 'span': '帮助用户表达自我，记录美好生活的短视频平台', 'offset': [6, 27]}]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 允许属性词缺省，#表示缺省\n",
    "semantic_cls(\n",
    "\tinput=app, \n",
    "  \tschema={\n",
    "        '属性词': {\n",
    "            '重点词': None,\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StructBert关键词抽取-中文-base-ICASSP2023-MUG-Track4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 23:01:57,238 - modelscope - INFO - Model revision not specified, use the latest revision: v1.0.0\n",
      "Downloading: 100%|██████████| 829/829 [00:00<00:00, 208kB/s]\n",
      "Downloading: 100%|██████████| 175/175 [00:00<00:00, 43.7kB/s]\n",
      "Downloading: 100%|██████████| 390M/390M [00:24<00:00, 16.6MB/s] \n",
      "Downloading: 100%|██████████| 2.86k/2.86k [00:00<00:00, 417kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 10.0kB/s]\n",
      "Downloading: 100%|██████████| 429k/429k [00:00<00:00, 2.66MB/s]\n",
      "Downloading: 100%|██████████| 364/364 [00:00<00:00, 28.0kB/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 877kB/s]\n",
      "2023-08-17 23:02:24,314 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline\n",
      "2023-08-17 23:02:24,317 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline.\n",
      "2023-08-17 23:02:24,325 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline\n",
      "2023-08-17 23:02:25,646 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "2023-08-17 23:02:25,845 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2023-08-17 23:02:25,845 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2023-08-17 23:02:25,853 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 23:02:25,853 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 23:02:25,854 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline'}. trying to build by task and model information.\n",
      "2023-08-17 23:02:25,871 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 23:02:25,872 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 23:02:25,872 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2023-08-17 23:02:25,885 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 23:02:25,886 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 23:02:25,886 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline', 'sequence_length': 512}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [{'type': 'KEY', 'start': 1, 'end': 3, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 22, 'end': 25, 'span': '短视频'},\n",
       "  {'type': 'KEY', 'start': 37, 'end': 39, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 90, 'end': 92, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 102, 'end': 104, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 145, 'end': 147, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 157, 'end': 159, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 207, 'end': 209, 'span': '抖音'}]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipeline(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
