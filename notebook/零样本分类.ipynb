{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改预训练模型的缓存目录\n",
    "import os\n",
    "\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = r\"G:\\code\\pretrain_model_dir\\_modelscope\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://modelscope.cn/topic/9ae88b6a1ffd4de59a9f1948314ebc2b/pub/summary\n",
    "\n",
    "# 我需要什么?\n",
    "\n",
    "我想要一个摘要和关键字抽取的功能, 用于从 app 描述信息里抽取功能描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抖音是一个帮助用户表达自我，记录美好生活的短视频平台。● 记录美好在抖音智能匹配音乐、一键卡点视频，还有超多原创特殊效果、滤镜、场景切换帮你一秒变大片，让你的生活轻松记录在抖音！● 实用内容在抖音生活妙招、美食做法、旅行攻略、科技知识、新闻时事、同城资讯，你需要的实用内容都在抖音！● 各行各业在抖音原创音乐人、京剧演员、中国科学院教授、非遗传承人、烧烤摊老板、快递小哥等，每个人真实的生活都在抖音！全民记录自我，生活的美好都在这里！＠联系我们-应用内反馈:「我」-「右上角菜单」-「设置」-「反馈与帮助」-官方邮箱：feedback@douyin.com-客户服务热线：95152\n"
     ]
    }
   ],
   "source": [
    "app = \"\"\"\n",
    "抖音是一个帮助用户表达自我，记录美好生活的短视频平台。\n",
    "\n",
    "● 记录美好在抖音\n",
    "智能匹配音乐、一键卡点视频，还有超多原创特殊效果、滤镜、场景切换帮你一秒变大片，让你的生活轻松记录在抖音！\n",
    "\n",
    "● 实用内容在抖音\n",
    "生活妙招、美食做法、旅行攻略、科技知识、新闻时事、同城资讯，你需要的实用内容都在抖音！\n",
    "\n",
    "● 各行各业在抖音\n",
    "原创音乐人、京剧演员、中国科学院教授、非遗传承人、烧烤摊老板、快递小哥等，每个人真实的生活都在抖音！\n",
    "\n",
    "全民记录自我，生活的美好都在这里！\n",
    "\n",
    "＠联系我们\n",
    "-应用内反馈:「我」-「右上角菜单」-「设置」-「反馈与帮助」\n",
    "-官方邮箱：feedback@douyin.com\n",
    "-客户服务热线：95152\n",
    "\"\"\".replace(\"\\n\", \"\")\n",
    "print(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我觉得关键词有:\n",
    "\n",
    "- 短视频平台\n",
    "- 记录生活\n",
    "- 分享知识\n",
    "- 全行业参与\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdd = \"\"\"\n",
    "拼多多——多实惠，多乐趣\n",
    "\n",
    "拼多多，社交新电商领导者，更懂消费者的手机购物APP。8亿用户在此拼团，享受更高性价比的生活。和你的家人、朋友、邻居一起拼，品质商品更低价，包邮送到家，分享实惠，分享乐趣。\n",
    "\n",
    "-多多买菜\n",
    "1、依托拼多多全新的农产品上行物流体系，一键直达全国超过1000个农产品产区。\n",
    "2、从田间到餐桌，避免中间环节损耗，百亿补贴加专项补贴，买菜更划算。\n",
    "3、手机变成菜篮子，精选全国及海外农产区产地优质好货，覆盖各类生活必需品。\n",
    "4、足不出户为家庭轻松选购次日生鲜，拿起手机选购，带着蔬果回家。\n",
    "\n",
    "-更懂生活，好货不必贵\n",
    "1、品类丰富：覆盖全品类品牌商品。小百货9.9包邮，轻松提升日常幸福感，品牌商品更低价，助你完成美好生活梦想。\n",
    "2、百亿补贴：精选补贴全网最热销品牌商品，直接降价，购物不必做算术题。\n",
    "3、产地好货：源头好货产地直达，从农田到餐桌，从车间到小区，中国制造四十年产业经验为你敞开。\n",
    "4、正品保障：全网首创假一赔十，品牌商品正品险，海关全程监督。\n",
    "\n",
    "-分享乐趣，购物不孤单\n",
    "1、发现好物，与好友共同分享，体验发现低价好货的乐趣。\n",
    "2、和志同道合的好友拼单，或参与万人团，与8亿拼多多用户一起拼。拼团享受更低价，组团购物不孤单。\n",
    "3、和朋友玩转小游戏。好朋友，就要一起栽种，一起收获。\n",
    "4、为好友呈现你的真实点评，为真朋友说出真体验。\n",
    "\n",
    "-产地直播，商家陪你看海\n",
    "1、看数百万真实商家产地直播。看红薯从地里挖出，看菠萝的海，看一双鞋如何从生产线走到你面前。\n",
    "2、市县长代言各产业带好货。市长县长精选本地产业带尖货，化身拼多多主播为你在线答疑。\n",
    "\n",
    "因为倾听，所以更懂你\n",
    "\n",
    "官方微信：拼多多（pinduoduo2015）\n",
    "官方微博：@拼多多\n",
    "客服电话：400-8822-528\n",
    "消费者固话热线：021-53395288\n",
    "官网：http://www.pinduoduo.com\n",
    "\"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 SiameseUniNLU零样本通用自然语言理解-中文-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:23:25,527 - modelscope - INFO - PyTorch version 2.0.0+cu118 Found.\n",
      "2023-08-17 22:23:25,529 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-08-17 22:23:25,530 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-08-17 22:23:25,530 - modelscope - INFO - No valid ast index found from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer, generating ast index from prebuilt!\n",
      "2023-08-17 22:23:25,652 - modelscope - INFO - Loading done! Current index file version is 1.8.3, with md5 f01211d2b2e03771df6500e25770738b and a total number of 895 components indexed\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-17 22:23:27,492 - modelscope - INFO - Use user-specified model revision: v1.0\n",
      "Downloading: 100%|██████████| 554/554 [00:00<00:00, 111kB/s]\n",
      "Downloading: 100%|██████████| 379/379 [00:00<00:00, 95.0kB/s]\n",
      "Downloading: 100%|██████████| 93.0k/93.0k [00:00<00:00, 1.13MB/s]\n",
      "Downloading: 100%|██████████| 390M/390M [00:14<00:00, 28.6MB/s] \n",
      "Downloading: 100%|██████████| 9.79k/9.79k [00:00<00:00, 2.00MB/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 749kB/s]\n",
      "2023-08-17 22:23:45,738 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uninlu_chinese-base\n",
      "2023-08-17 22:23:45,739 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uninlu_chinese-base.\n",
      "2023-08-17 22:23:45,741 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uninlu_chinese-base\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "semantic_cls = pipeline(Tasks.siamese_uie, 'damo/nlp_structbert_siamese-uninlu_chinese-base', model_revision='v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_cls.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '人物', 'span': '谷口清太郎', 'offset': [18, 23]}],\n",
       "  [{'type': '组织机构', 'span': '北大', 'offset': [8, 10]}],\n",
       "  [{'type': '组织机构', 'span': '名古屋铁道', 'offset': [11, 16]}]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 命名实体识别 {实体类型: None}\n",
    "semantic_cls(\n",
    "    input='1944年毕业于北大的名古屋铁道会长谷口清太郎等人在日本积极筹资，共筹款2.7亿日元，参加捐款的日本企业有69家。', \n",
    "    schema={\n",
    "        '人物': None,\n",
    "        '地理位置': None,\n",
    "        '组织机构': None\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '人物', 'span': '谷爱凌', 'offset': [32, 35]},\n",
       "   {'type': '比赛项目(赛事名称)', 'span': '滑雪女子大跳台决赛', 'offset': [18, 27]}],\n",
       "  [{'type': '人物', 'span': '谷爱凌', 'offset': [32, 35]},\n",
       "   {'type': '比赛项目(赛事名称)', 'span': '滑雪男子大跳台决赛', 'offset': [55, 64]}],\n",
       "  [{'type': '人物', 'span': '谷爱凌', 'offset': [32, 35]},\n",
       "   {'type': '选手国籍(国籍)', 'span': '中国', 'offset': [28, 30]}],\n",
       "  [{'type': '人物', 'span': '小泉次郎', 'offset': [69, 73]},\n",
       "   {'type': '比赛项目(赛事名称)', 'span': '滑雪女子大跳台决赛', 'offset': [18, 27]}],\n",
       "  [{'type': '人物', 'span': '小泉次郎', 'offset': [69, 73]},\n",
       "   {'type': '比赛项目(赛事名称)', 'span': '滑雪男子大跳台决赛', 'offset': [55, 64]}],\n",
       "  [{'type': '人物', 'span': '小泉次郎', 'offset': [69, 73]},\n",
       "   {'type': '选手国籍(国籍)', 'span': '中国', 'offset': [28, 30]}]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 关系抽取 {主语实体类型: {关系(宾语实体类型): None}}\n",
    "semantic_cls(\n",
    "\tinput='在北京冬奥会自由式中，2月8日上午，滑雪女子大跳台决赛中中国选手谷爱凌以188.25分获得金牌。2月9日上午，滑雪男子大跳台决赛中日本选手小泉次郎以188.25分获得银牌！', \n",
    "  \tschema={\n",
    "        '人物': {\n",
    "            '比赛项目(赛事名称)': None,\n",
    "            '参赛地点(城市)': None,\n",
    "            '获奖时间(时间)': None,\n",
    "            '选手国籍(国籍)': None\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '胜负(事件触发词)', 'span': '负于', 'offset': [19, 21]},\n",
       "   {'type': '时间', 'span': '7月28日', 'offset': [0, 5]}],\n",
       "  [{'type': '胜负(事件触发词)', 'span': '负于', 'offset': [19, 21]},\n",
       "   {'type': '败者', 'span': '天津泰达', 'offset': [6, 10]}],\n",
       "  [{'type': '胜负(事件触发词)', 'span': '负于', 'offset': [19, 21]},\n",
       "   {'type': '胜者', 'span': '天津泰达', 'offset': [6, 10]}],\n",
       "  [{'type': '胜负(事件触发词)', 'span': '负于', 'offset': [19, 21]},\n",
       "   {'type': '胜者', 'span': '天津天海', 'offset': [21, 25]}]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 事件抽取 {事件类型（事件触发词）: {参数类型: None}}\n",
    "semantic_cls(\n",
    "\tinput='7月28日，天津泰达在德比战中以0-1负于天津天海。', \n",
    "  \tschema={\n",
    "        '胜负(事件触发词)': {\n",
    "            '时间': None,\n",
    "            '败者': None,\n",
    "            '胜者': None,\n",
    "            '赛事名称': None\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '属性词', 'span': '音质', 'offset': [4, 6]},\n",
       "   {'type': '情感词', 'span': '很好', 'offset': [6, 8]}],\n",
       "  [{'type': '属性词', 'span': '音质', 'offset': [4, 6]},\n",
       "   {'type': '情感词', 'span': '快', 'offset': [13, 14]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [9, 13]},\n",
       "   {'type': '情感词', 'span': '快', 'offset': [13, 14]}]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 属性情感抽取 {属性词: {情感词: None}}\n",
    "semantic_cls(\n",
    "\tinput='很满意，音质很好，发货速度快，值得购买', \n",
    "  \tschema={\n",
    "        '属性词': {\n",
    "            '情感词': None,\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '属性词', 'span': '#', 'offset': [0, 1]},\n",
       "   {'type': '情感词', 'span': '很满意', 'offset': [1, 4]}],\n",
       "  [{'type': '属性词', 'span': '#', 'offset': [0, 1]},\n",
       "   {'type': '情感词', 'span': '值得购买', 'offset': [16, 20]}],\n",
       "  [{'type': '属性词', 'span': '音质', 'offset': [5, 7]},\n",
       "   {'type': '情感词', 'span': '很好', 'offset': [7, 9]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [10, 14]},\n",
       "   {'type': '情感词', 'span': '快', 'offset': [14, 15]}]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 允许属性词缺省，#表示缺省\n",
    "semantic_cls(\n",
    "\tinput='#很满意，音质很好，发货速度快，值得购买', \n",
    "  \tschema={\n",
    "        '属性词': {\n",
    "            '情感词': None,\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '属性词', 'span': '音质', 'offset': [4, 6]},\n",
       "   {'type': '正向情感(情感词)', 'span': '很好', 'offset': [6, 8]}],\n",
       "  [{'type': '属性词', 'span': '音质', 'offset': [4, 6]},\n",
       "   {'type': '正向情感(情感词)', 'span': '快', 'offset': [13, 14]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [9, 13]},\n",
       "   {'type': '正向情感(情感词)', 'span': '很满意', 'offset': [0, 3]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [9, 13]},\n",
       "   {'type': '正向情感(情感词)', 'span': '快', 'offset': [13, 14]}],\n",
       "  [{'type': '属性词', 'span': '发货速度', 'offset': [9, 13]},\n",
       "   {'type': '正向情感(情感词)', 'span': '值得购买', 'offset': [15, 19]}]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 支持情感分类\n",
    "semantic_cls(\n",
    "\tinput='很满意，音质很好，发货速度快，值得购买', \n",
    "  \tschema={\n",
    "        '属性词': {\n",
    "            \"正向情感(情感词)\": None, \n",
    "            \"负向情感(情感词)\": None, \n",
    "            \"中性情感(情感词)\": None\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '在下面的描述中，代词“她”指代的是“妹妹”吗？',\n",
       "    'span': '不是',\n",
       "    'offset': [3, 5]}]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指代消解，判断选项通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔\n",
    "semantic_cls(\n",
    "\tinput='是的,不是|哥哥点了点头。“我这几年苦哇……现在玲玲也大一点了，所以……”他望着妹妹（候选词），脸上显出一副要求她(代词)谅解的表情。', \n",
    "  \tschema={\n",
    "        '在下面的描述中，代词“她”指代的是“妹妹”吗？': None\n",
    "        }\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '情感分类', 'span': '负向', 'offset': [3, 5]}]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 情感分类，情感标签通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔；同时也支持情绪分类任务，换成相应情绪标签即可，e.g. \"无情绪,积极,愤怒,悲伤,恐惧,惊奇\"\n",
    "semantic_cls(\n",
    "\tinput='正向,负向|有点看不下去了，看作者介绍就觉得挺矫情了，文字也弱了点。后来才发现 大家对这本书评价都很低。亏了。', \n",
    "  \tschema={\n",
    "        '情感分类': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '分类', 'span': '教育', 'offset': [23, 25]}]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本分类，文本标签通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔\n",
    "semantic_cls(\n",
    "\tinput='民生故事,文化,娱乐,体育,财经,房产,汽车,教育,科技,军事,旅游,国际,证券股票,农业三农,电竞游戏|学校召开2018届升学及出国深造毕业生座谈会就业指导', \n",
    "  \tschema={\n",
    "        '分类': None\n",
    "        }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '文本匹配', 'span': '不相似', 'offset': [3, 6]}]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本匹配，文本相似度标签通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔；输入文本由两段文本组成，并用“&”隔开\n",
    "semantic_cls(\n",
    "\tinput='相似,不相似|摄像头区域遮挡屏幕&通话遮挡屏幕黑屏正常', \n",
    "  \tschema={\n",
    "        '文本匹配': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '下面两句话的意思是否相同', 'span': '是的', 'offset': [0, 2]}],\n",
       "  [{'type': '下面两句话的意思是否相同', 'span': '不是', 'offset': [3, 5]}]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本匹配也可以用下面这种方式组织，判断选项通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔；输入文本由两段文本组成，并分别用“句子1”和“句子2”区分\n",
    "semantic_cls(\n",
    "\tinput='是的,不是|句子1：摄像头区域遮挡屏幕；句子2：通话遮挡屏幕黑屏正常', \n",
    "  \tschema={\n",
    "        '下面两句话的意思是否相同': None\n",
    "        }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '段落2和段落1的关系是：', 'span': '矛盾', 'offset': [3, 5]}]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自然语言推理，文本关系标签通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔；输入文本由两段文本组成，并分别用“段落1”和“段落2”区分\n",
    "semantic_cls(\n",
    "\tinput='蕴含,矛盾,中立|段落1：是,但是你比如说像现在这种情况,是不是就是说咱们离它就绝对人类是再也没有任何可能性了；段落2：我对人类可能性有所思考', \n",
    "  \tschema={\n",
    "        '段落2和段落1的关系是：': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': 'B为什么不坐飞机?', 'span': '坐飞机头晕', 'offset': [12, 17]}]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择类阅读理解，选项通过英文逗号“,”隔开，拼接在输入文本前面并用“｜”分隔\n",
    "semantic_cls(\n",
    "\tinput='飞机票太贵,时间来不及,坐飞机头晕,飞机票太便宜|A：最近飞机票打折挺多的，你还是坐飞机去吧。B：反正又不是时间来不及，飞机再便宜我也不坐，我一听坐飞机就头晕。', \n",
    "  \tschema={\n",
    "        'B为什么不坐飞机?': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '大莱龙铁路位于哪里？', 'span': '山东', 'offset': [7, 9]}]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 抽取类阅读理解\n",
    "semantic_cls(\n",
    "\tinput='大莱龙铁路位于山东省北部环渤海地区，西起位于益羊铁路的潍坊大家洼车站，向东经海化、寿光、寒亭、昌邑、平度、莱州、招远、终到龙口，连接山东半岛羊角沟、潍坊、莱州、龙口四个港口，全长175公里，工程建设概算总投资11.42亿元。铁路西与德大铁路、黄大铁路在大家洼站接轨，东与龙烟铁路相连。大莱龙铁路于1997年11月批复立项，2002年12月28日全线铺通，2005年6月建成试运营，是横贯山东省北部的铁路干线德龙烟铁路的重要组成部分，构成山东省北部沿海通道，并成为环渤海铁路网的南部干线。铁路沿线设有大家洼站、寒亭站、昌邑北站、海天站、平度北站、沙河站、莱州站、朱桥站、招远站、龙口西站、龙口北站、龙口港站。大莱龙铁路官方网站', \n",
    "  \tschema={\n",
    "        '大莱龙铁路位于哪里？': None\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全任务零样本学习-mT5分类增强版-中文-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 20:25:59,711 - modelscope - INFO - Use user-specified model revision: v1.0.0\n",
      "2023-08-20 20:25:59,952 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_mt5_zero-shot-augment_chinese-base\n",
      "2023-08-20 20:25:59,953 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_mt5_zero-shot-augment_chinese-base.\n",
      "2023-08-20 20:25:59,956 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_mt5_zero-shot-augment_chinese-base\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "2023-08-20 20:26:06,388 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-20 20:26:06,389 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-20 20:26:06,389 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_mt5_zero-shot-augment_chinese-base'}. trying to build by task and model information.\n",
      "2023-08-20 20:26:06,408 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-20 20:26:06,409 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-20 20:26:06,409 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_mt5_zero-shot-augment_chinese-base', 'first_sequence': 'sentence'}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "\n",
    "t2t_generator = pipeline(\"text2text-generation\", \"damo/nlp_mt5_zero-shot-augment_chinese-base\", model_revision=\"v1.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:35:16,613 - modelscope - WARNING - task text2text-generation input definition is missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '文化'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(t2t_generator(\"文本分类。\\n候选标签：故事,房产,娱乐,文化,游戏,国际,股票,科技,军事,教育。\\n文本内容：他们的故事平静而闪光，一代人奠定沉默的基石，让中国走向繁荣。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '无线Mesh网,路由协议,环境感知推理'}\n"
     ]
    }
   ],
   "source": [
    "print(t2t_generator(\"抽取关键词：\\n在分析无线Mesh网路由协议所面临挑战的基础上,结合无线Mesh网络的性能要求,以优化链路状态路由(OLSR)协议为原型,采用跨层设计理论,提出了一种基于链路状态良好程度的路由协议LR-OLSR.该协议引入了认知无线网络中的环境感知推理思想,通过时节点负载、链路投递率和链路可用性等信息进行感知,并以此为依据对链路质量进行推理,获得网络中源节点和目的节点对之间各路径状态良好程度的评价,将其作为路由选择的依据,实现对路由的优化选择,提高网络的吞吐量,达到负载均衡.通过与OLSR及其典型改进协议P-OLSR、SC-OLSR的对比仿真结果表明,LR-OLSB能够提高网络中分组的递交率,降低平均端到端时延,在一定程度上达到负载均衡.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '短视频平台,科技知识,短视频平台,短视频,短视频,中国'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"抽取关键词：\\n{app}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '电商领导者,农产品产区,电商APP,电商领导者,电商领导者'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"提取关键字：\\n{pdd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '短视频平台,短视频,生活妙招,科技知识,新闻时事,'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"提取重点：\\n{app}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '电商领导者,农产品产区,电商APP,电商领导者,电商领导者'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"提取关键短语：\\n{pdd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '短视频,抖音'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"用关键词概述并总结：\\n{app}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '拼多多,农产品上行物流,电商领导者,电商领导者,电商领导'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2t_generator(f\"用关键词概述并总结：\\n{pdd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SiameseUIE通用信息抽取-中文-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 22:44:09,173 - modelscope - INFO - Use user-specified model revision: v1.0\n",
      "Downloading: 100%|██████████| 554/554 [00:00<00:00, 92.5kB/s]\n",
      "Downloading: 100%|██████████| 379/379 [00:00<00:00, 54.1kB/s]\n",
      "Downloading: 100%|██████████| 55.8k/55.8k [00:00<00:00, 883kB/s]\n",
      "Downloading: 100%|██████████| 32.6k/32.6k [00:00<00:00, 755kB/s]\n",
      "Downloading: 100%|██████████| 390M/390M [00:14<00:00, 28.5MB/s] \n",
      "Downloading: 100%|██████████| 2.24k/2.24k [00:00<00:00, 458kB/s]\n",
      "Downloading: 100%|██████████| 7.24k/7.24k [00:00<00:00, 831kB/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 942kB/s] \n",
      "Downloading: 100%|██████████| 68.2k/68.2k [00:00<00:00, 932kB/s]\n",
      "2023-08-17 22:44:26,196 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uie_chinese-base\n",
      "2023-08-17 22:44:26,196 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uie_chinese-base.\n",
      "2023-08-17 22:44:26,198 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_siamese-uie_chinese-base\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "semantic_cls = pipeline(Tasks.siamese_uie, 'damo/nlp_structbert_siamese-uie_chinese-base', model_revision='v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [[{'type': '属性词', 'span': '抖音', 'offset': [1, 3]},\n",
       "   {'type': '重点词', 'span': '帮助用户表达自我，记录美好生活的短视频平台', 'offset': [6, 27]}]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 允许属性词缺省，#表示缺省\n",
    "semantic_cls(\n",
    "\tinput=app, \n",
    "  \tschema={\n",
    "        '属性词': {\n",
    "            '重点词': None,\n",
    "        }\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StructBert关键词抽取-中文-base-ICASSP2023-MUG-Track4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 23:01:57,238 - modelscope - INFO - Model revision not specified, use the latest revision: v1.0.0\n",
      "Downloading: 100%|██████████| 829/829 [00:00<00:00, 208kB/s]\n",
      "Downloading: 100%|██████████| 175/175 [00:00<00:00, 43.7kB/s]\n",
      "Downloading: 100%|██████████| 390M/390M [00:24<00:00, 16.6MB/s] \n",
      "Downloading: 100%|██████████| 2.86k/2.86k [00:00<00:00, 417kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 10.0kB/s]\n",
      "Downloading: 100%|██████████| 429k/429k [00:00<00:00, 2.66MB/s]\n",
      "Downloading: 100%|██████████| 364/364 [00:00<00:00, 28.0kB/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 877kB/s]\n",
      "2023-08-17 23:02:24,314 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline\n",
      "2023-08-17 23:02:24,317 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline.\n",
      "2023-08-17 23:02:24,325 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline\n",
      "2023-08-17 23:02:25,646 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "2023-08-17 23:02:25,845 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTokenClassificationWithCRF.\n",
      "\n",
      "2023-08-17 23:02:25,845 - modelscope - INFO - All the weights of ModelForTokenClassificationWithCRF were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTokenClassificationWithCRF for predictions without further training.\n",
      "2023-08-17 23:02:25,853 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 23:02:25,853 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 23:02:25,854 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline'}. trying to build by task and model information.\n",
      "2023-08-17 23:02:25,871 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 23:02:25,872 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 23:02:25,872 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline', 'sequence_length': 512}. trying to build by task and model information.\n",
      "2023-08-17 23:02:25,885 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 23:02:25,886 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 23:02:25,886 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline', 'sequence_length': 512}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_structbert_keyphrase-extraction_base-icassp2023-mug-track4-baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [{'type': 'KEY', 'start': 1, 'end': 3, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 22, 'end': 25, 'span': '短视频'},\n",
       "  {'type': 'KEY', 'start': 37, 'end': 39, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 90, 'end': 92, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 102, 'end': 104, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 145, 'end': 147, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 157, 'end': 159, 'span': '抖音'},\n",
       "  {'type': 'KEY', 'start': 207, 'end': 209, 'span': '抖音'}]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipeline(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoNet抽取式话题摘要模型-中文-base-ICASSP2023-MUG-Track2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 23:18:38,923 - modelscope - INFO - Model revision not specified, use the latest revision: v1.0.0\n",
      "Downloading: 100%|██████████| 951/951 [00:00<00:00, 191kB/s]\n",
      "Downloading: 100%|██████████| 307/307 [00:00<00:00, 102kB/s]\n",
      "Downloading: 100%|██████████| 453M/453M [00:16<00:00, 28.4MB/s] \n",
      "Downloading: 100%|██████████| 1.49k/1.49k [00:00<00:00, 170kB/s]\n",
      "Downloading: 100%|██████████| 134/134 [00:00<00:00, 22.4kB/s]\n",
      "Downloading: 100%|██████████| 263k/263k [00:00<00:00, 1.36MB/s]\n",
      "Downloading: 100%|██████████| 518/518 [00:00<00:00, 74.1kB/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 797kB/s]\n",
      "2023-08-17 23:18:58,681 - modelscope - INFO - initiate model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_ponet_extractive-summarization_topic-level_chinese-base\n",
      "2023-08-17 23:18:58,681 - modelscope - INFO - initiate model from location G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_ponet_extractive-summarization_topic-level_chinese-base.\n",
      "2023-08-17 23:18:58,684 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\damo\\nlp_ponet_extractive-summarization_topic-level_chinese-base\n",
      "You are using a model of type bert to instantiate a model of type ponet. This is not supported for all configurations of models and can yield errors.\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "2023-08-17 23:19:00,003 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-08-17 23:19:00,003 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-17 23:19:00,004 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\damo\\\\nlp_ponet_extractive-summarization_topic-level_chinese-base'}. trying to build by task and model information.\n",
      "2023-08-17 23:19:00,004 - modelscope - WARNING - No preprocessor key ('ponet-for-document-segmentation', 'extractive-summarization') found in PREPROCESSOR_MAP, skip building preprocessor.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.outputs import OutputKeys\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "p = pipeline(\n",
    "    task=Tasks.extractive_summarization,\n",
    "    model='damo/nlp_ponet_extractive-summarization_topic-level_chinese-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输入为Fbank特征，输出为基于char建模的中文全集token预测，测试工具根据每一帧的预测数据进行后处理得到输入音频的实时检测结果。\n",
      "模型训练采用“basetrain + finetune”的模式，basetrain过程使用大量内部移动端数据，在此基础上，使用1万条设备端录制安静场景“小云小云”数据进行微调，得到最终面向业务的模型。\n"
     ]
    }
   ],
   "source": [
    "result = p(documents='移动端语音唤醒模型，检测关键词为“小云小云”。模型主体为4层FSMN结构，使用CTC训练准则，参数量750K，适用于移动端设备运行。模型输入为Fbank特征，输出为基于char建模的中文全集token预测，测试工具根据每一帧的预测数据进行后处理得到输入音频的实时检测结果。模型训练采用“basetrain + finetune”的模式，basetrain过程使用大量内部移动端数据，在此基础上，使用1万条设备端录制安静场景“小云小云”数据进行微调，得到最终面向业务的模型。后续用户可在basetrain模型基础上，使用其他关键词数据进行微调，得到新的语音唤醒模型，但暂时未开放模型finetune功能。')\n",
    "\n",
    "print(result[OutputKeys.TEXT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全中文任务支持零样本学习模型v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 23:21:54,962 - modelscope - INFO - Use user-specified model revision: v0.1\n",
      "Downloading: 100%|██████████| 726/726 [00:00<00:00, 145kB/s]\n",
      "Downloading: 100%|██████████| 199/199 [00:00<00:00, 49.8kB/s]\n",
      "Downloading: 100%|██████████| 945M/945M [00:39<00:00, 24.9MB/s] \n",
      "Downloading: 100%|██████████| 12.1k/12.1k [00:00<00:00, 410kB/s]\n",
      "Downloading: 100%|██████████| 725k/725k [00:00<00:00, 3.16MB/s]\n",
      "Downloading: 100%|██████████| 520k/520k [00:00<00:00, 3.15MB/s]\n",
      "2023-08-17 23:22:36,968 - modelscope - INFO - initialize model from G:\\code\\pretrain_model_dir\\_modelscope\\ClueAI\\PromptCLUE-base-v1-5\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.models.nlp import T5ForConditionalGeneration\n",
    "from modelscope.preprocessors import TextGenerationTransformersPreprocessor\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('ClueAI/PromptCLUE-base-v1-5', revision='v0.1')\n",
    "preprocessor = TextGenerationTransformersPreprocessor(model.model_dir)\n",
    "pipeline_t2t = pipeline(task=Tasks.text2text_generation, model=model, preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 23:24:15,716 - modelscope - WARNING - task text2text-generation input definition is missing\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '机构：杭州线锁科技_人名：张玄武_职位：博士学历，'}\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_t2t(\"阅读文本抽取关键信息：\\n张玄武1990年出生中国国籍无境外居留权博士学历现任杭州线锁科技技术总监。\\n问题：机构，人名，职位，籍贯，专业，国籍，学历，种族\\n答案：\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '抖音是一个帮助用户表达自我，记录美好生活的短视频平台。'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_t2t(f\"阅读文本抽取关键信息：\\n{app}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:830: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:784: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '抖音，音乐，视频，生活，抖音'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_t2t(f\"提取关键词：\\n{app}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mGLM多语言大模型-生成式摘要-中文\n",
    "\n",
    "TODO: 这个不行, 需要 deepspeed, 然后不支持 windows, 只支持一个推理模式.\n",
    "\n",
    "https://github.com/microsoft/DeepSpeed#windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple\n",
      "Looking in links: https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n",
      "Collecting megatron_util\n",
      "  Downloading https://modelscope.oss-cn-beijing.aliyuncs.com/releases/dependencies/megatron_util-1.3.2-py3-none-any.whl (182 kB)\n",
      "     -------------------------------------- 183.0/183.0 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: megatron_util\n",
      "Successfully installed megatron_util-1.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install megatron_util -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple\n",
      "Collecting deepspeed\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a9/4f/d37d2f0bd05f4504370eb0a763c0dbdba12ad12daeb3104c00ac416faeac/deepspeed-0.10.1.tar.gz (851 kB)\n",
      "     -------------------------------------- 851.5/851.5 kB 3.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      DS_BUILD_OPS=1\n",
      "      test.c\n",
      "      LINK : fatal error LNK1181: \\xce޷\\xa8\\xb4\\xf2\\xbf\\xaa\\xca\\xe4\\xc8\\xeb\\xceļ\\xfe\\xa1\\xb0aio.lib\\xa1\\xb1\n",
      "      \u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "      \u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "      \u001b[93m [WARNING] \u001b[0m One can disable async_io with DS_BUILD_AIO=0\n",
      "      \u001b[31m [ERROR] \u001b[0m Unable to pre-compile async_io\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\zhenh\\AppData\\Local\\Temp\\pip-install-28hm2prd\\deepspeed_e53b2f555d2844a88243b600e749bbbb\\setup.py\", line 165, in <module>\n",
      "          abort(f\"Unable to pre-compile {op_name}\")\n",
      "        File \"C:\\Users\\zhenh\\AppData\\Local\\Temp\\pip-install-28hm2prd\\deepspeed_e53b2f555d2844a88243b600e749bbbb\\setup.py\", line 51, in abort\n",
      "          assert False, msg\n",
      "      AssertionError: Unable to pre-compile async_io\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 20:55:30,120 - modelscope - INFO - Use user-specified model revision: v1.0.1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import modelscope.models.nlp.mglm.mglm_for_text_summarization because of the following error (look up to see its traceback):\nNo module named 'deepspeed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\utils\\import_utils.py:435\u001b[0m, in \u001b[0;36mLazyImportModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    434\u001b[0m         requires(module_name_full, requirements)\n\u001b[1;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m    436\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\models\\nlp\\mglm\\mglm_for_text_summarization.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmegatron_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m init_megatron_util\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marguments\u001b[39;00m \u001b[39mimport\u001b[39;00m get_args\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgeneration_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m BeamSearchScorer\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\models\\nlp\\mglm\\arguments.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdeepspeed\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepspeed'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mg:\\code\\github\\modelscope\\notebook\\零样本分类.ipynb 单元格 50\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/code/github/modelscope/notebook/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mZhipuAI/Multilingual-GLM-Summarization-zh\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/code/github/modelscope/notebook/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m preprocessor \u001b[39m=\u001b[39m MGLMSummarizationPreprocessor()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/code/github/modelscope/notebook/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/code/github/modelscope/notebook/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     task\u001b[39m=\u001b[39;49mTasks\u001b[39m.\u001b[39;49mtext_summarization,\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/code/github/modelscope/notebook/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb#X60sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/code/github/modelscope/notebook/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     preprocessor\u001b[39m=\u001b[39;49mpreprocessor,\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/code/github/modelscope/notebook/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model_revision\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mv1.0.1\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/code/github/modelscope/notebook/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\pipelines\\builder.py:147\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, preprocessor, config_file, pipeline_name, framework, device, model_revision, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     cfg\u001b[39m.\u001b[39mpreprocessor \u001b[39m=\u001b[39m preprocessor\n\u001b[1;32m--> 147\u001b[0m \u001b[39mreturn\u001b[39;00m build_pipeline(cfg, task_name\u001b[39m=\u001b[39;49mtask)\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\pipelines\\builder.py:59\u001b[0m, in \u001b[0;36mbuild_pipeline\u001b[1;34m(cfg, task_name, default_args)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_pipeline\u001b[39m(cfg: ConfigDict,\n\u001b[0;32m     49\u001b[0m                    task_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     50\u001b[0m                    default_args: \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     51\u001b[0m     \u001b[39m\"\"\" build pipeline given model config dict.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m        default_args (dict, optional): Default initialization arguments.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m build_from_cfg(\n\u001b[0;32m     60\u001b[0m         cfg, PIPELINES, group_key\u001b[39m=\u001b[39;49mtask_name, default_args\u001b[39m=\u001b[39;49mdefault_args)\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\utils\\registry.py:184\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[1;34m(cfg, registry, group_key, default_args)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyImportModule\n\u001b[0;32m    183\u001b[0m sig \u001b[39m=\u001b[39m (registry\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mupper(), group_key, cfg[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 184\u001b[0m LazyImportModule\u001b[39m.\u001b[39;49mimport_module(sig)\n\u001b[0;32m    186\u001b[0m args \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m default_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\utils\\import_utils.py:459\u001b[0m, in \u001b[0;36mLazyImportModule.import_module\u001b[1;34m(signature)\u001b[0m\n\u001b[0;32m    456\u001b[0m         requirements \u001b[39m=\u001b[39m LazyImportModule\u001b[39m.\u001b[39mAST_INDEX[REQUIREMENT_KEY][\n\u001b[0;32m    457\u001b[0m             module_name]\n\u001b[0;32m    458\u001b[0m         requires(module_name, requirements)\n\u001b[1;32m--> 459\u001b[0m     importlib\u001b[39m.\u001b[39;49mimport_module(module_name)\n\u001b[0;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msignature\u001b[39m}\u001b[39;00m\u001b[39m not found in ast index file\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\pipelines\\nlp\\mglm_text_summarization_pipeline.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetainfo\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipelines\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnlp\u001b[39;00m \u001b[39mimport\u001b[39;00m MGLMForTextSummarization\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline, Tensor\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodelscope\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m PIPELINES\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\utils\\import_utils.py:419\u001b[0m, in \u001b[0;36mLazyImportModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    418\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[1;32m--> 419\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[0;32m    420\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\utils\\import_utils.py:418\u001b[0m, in \u001b[0;36mLazyImportModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    416\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[0;32m    417\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 418\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m    419\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m    420\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\modelscope\\utils\\import_utils.py:437\u001b[0m, in \u001b[0;36mLazyImportModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    436\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(look up to see its traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import modelscope.models.nlp.mglm.mglm_for_text_summarization because of the following error (look up to see its traceback):\nNo module named 'deepspeed'"
     ]
    }
   ],
   "source": [
    "from modelscope.models import Model\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.preprocessors import MGLMSummarizationPreprocessor\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "model = 'ZhipuAI/Multilingual-GLM-Summarization-zh'\n",
    "preprocessor = MGLMSummarizationPreprocessor()\n",
    "pipe = pipeline(\n",
    "    task=Tasks.text_summarization,\n",
    "    model=model,\n",
    "    preprocessor=preprocessor,\n",
    "    model_revision='v1.0.1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
