{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改预训练模型的缓存目录\n",
    "import os\n",
    "\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = r\"G:\\code\\pretrain_model_dir\\_modelscope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-16 22:57:27,396 - modelscope - INFO - PyTorch version 1.13.1+cu117 Found.\n",
      "2023-10-16 22:57:27,399 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-10-16 22:57:27,399 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-10-16 22:57:27,648 - modelscope - INFO - Loading done! Current index file version is 1.8.3, with md5 f01211d2b2e03771df6500e25770738b and a total number of 895 components indexed\n",
      "2023-10-16 22:57:29,431 - modelscope - INFO - Use user-specified model revision: v1.0.5\n",
      "2023-10-16 22:57:31,708 - modelscope - WARNING - ('PIPELINES', 'text-generation', 'Baichuan-7B-text-generation-pipe') not found in ast index file\n",
      "2023-10-16 22:57:58,596 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-10-16 22:57:58,597 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-10-16 22:57:58,597 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\baichuan-inc\\\\baichuan-7B'}. trying to build by task and model information.\n",
      "2023-10-16 22:57:58,598 - modelscope - WARNING - No preprocessor key ('Baichuan-7B', 'text-generation') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
      "2023-10-16 22:57:58,601 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-10-16 22:57:58,601 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-10-16 22:57:58,601 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\baichuan-inc\\\\baichuan-7B', 'first_sequence': 'sentence', 'device_map': 'auto'}. trying to build by task and model information.\n",
      "2023-10-16 22:57:58,602 - modelscope - WARNING - No preprocessor key ('Baichuan-7B', 'text-generation') found in PREPROCESSOR_MAP, skip building preprocessor.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.pipelines import pipeline\n",
    "text_generation_zh  = pipeline(task=Tasks.text_generation, model='baichuan-inc/baichuan-7B', device_map='auto',model_revision='v1.0.5')\n",
    "text_generation_zh._model_prepare = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '今天天气是真的好,阳光明媚,风和日丽,心情也跟着好了起来。 下午的时候,我正在家里看电视,忽然听见有人敲门,我打开门一看,原来是老爸回来了。 老爸一进门就说:“儿子,你妈呢?” 我说:“她出去买菜了。” 老爸说:“那你去帮我买包烟吧。” 我说:“好吧。” 于是我就去楼下的小卖部买烟。 到了小卖部,我对老板说:“老板,给我来包玉溪。” 老板说:“好嘞。” 我付完钱,正准备走的时候,老板忽然叫住我说:“小伙子,你还没给钱呢。” 我说:“什么钱?” 老板说:“你刚才买烟的钱。” 我说:“我买烟的时候,你不是说好嘞吗?怎么现在又说没给钱呢?” 老板笑着说:“我说的是好嘞,不是没给钱。” 我......'}\n"
     ]
    }
   ],
   "source": [
    "result_zh = text_generation_zh('今天天气是真的', min_length=10, max_length=512, num_beams=3,temperature=0.8,do_sample=False, early_stopping=True,top_k=50,top_p=0.8, repetition_penalty=1.2, length_penalty=1.2, no_repeat_ngram_size=6)\n",
    "print(result_zh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baichuan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-21 17:04:13,734 - modelscope - INFO - PyTorch version 1.13.1+cu117 Found.\n",
      "2023-10-21 17:04:13,737 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-10-21 17:04:13,737 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-10-21 17:04:13,931 - modelscope - INFO - Loading done! Current index file version is 1.8.3, with md5 f01211d2b2e03771df6500e25770738b and a total number of 895 components indexed\n",
      "2023-10-21 17:04:16,072 - modelscope - INFO - Use user-specified model revision: v1.0.3\n",
      "Downloading: 100%|██████████| 716/716 [00:00<00:00, 179kB/s]\n",
      "Downloading: 100%|██████████| 217/217 [00:00<00:00, 54.3kB/s]\n",
      "Downloading: 100%|██████████| 1.52k/1.52k [00:00<00:00, 386kB/s]\n",
      "Downloading: 100%|██████████| 285/285 [00:00<00:00, 55.0kB/s]\n",
      "Downloading: 100%|██████████| 2.90k/2.90k [00:00<00:00, 593kB/s]\n",
      "Downloading: 100%|██████████| 31.8k/31.8k [00:00<00:00, 760kB/s]\n",
      "Downloading: 100%|██████████| 3.28k/3.28k [00:00<00:00, 671kB/s]\n",
      "Downloading: 100%|██████████| 9.29G/9.29G [05:48<00:00, 28.6MB/s]\n",
      "Downloading: 100%|██████████| 9.26G/9.26G [05:44<00:00, 28.9MB/s]\n",
      "Downloading: 100%|██████████| 7.33G/7.33G [04:30<00:00, 29.1MB/s]\n",
      "Downloading: 100%|██████████| 22.7k/22.7k [00:00<00:00, 618kB/s]\n",
      "Downloading: 100%|██████████| 8.97k/8.97k [00:00<00:00, 1.84MB/s]\n",
      "Downloading: 100%|██████████| 10.7k/10.7k [00:00<00:00, 2.18MB/s]\n",
      "Downloading: 100%|██████████| 544/544 [00:00<00:00, 68.0kB/s]\n",
      "Downloading: 100%|██████████| 8.82k/8.82k [00:00<00:00, 821kB/s]\n",
      "Downloading: 100%|██████████| 1.91M/1.91M [00:00<00:00, 6.56MB/s]\n",
      "Downloading: 100%|██████████| 954/954 [00:00<00:00, 191kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\code\\pretrain_model_dir\\_modelscope\\baichuan-inc\\Baichuan2-13B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModelForCausalLM, AutoTokenizer,GenerationConfig\n",
    "model_dir = snapshot_download(\"baichuan-inc/Baichuan2-13B-Chat\", revision='v1.0.3')\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"温故而知新\"是一句源自《论语·为政》的古语，这句话的意思是：通过回顾过去的学习和经历，从而获得新的理解和感悟。这句话强调了学习和成长的一个重要方法，即不断地回顾和反思过去的知识、经验和人际交往，以便从中汲取新的智慧和启示。\n",
      "\n",
      "具体来说，\"温故\"意味着回顾过去，重温学过的知识、做过的决策和经历过的事情。这个过程可以帮助我们更好地理解自己的成长过程，发现自己的优点和不足，以及可能在之前忽略的线索和规律。\n",
      "\n",
      "而\"知新\"则是指在这个过程中，我们发现新的见解、灵感或者解决方案。这些新的认识可能来自于对过去经验的重新审视，也可能来自于与其他人和事物的互动。总之，\"知新\"意味着我们在回顾过去的基础上，实现了某种程度的自我提升和成长。\n",
      "\n",
      "总的来说，\"温故而知新\"是一种倡导终身学习的精神，它鼓励我们在人生的各个阶段都要保持对新知识的探索和对旧经验的反思，从而实现持续的成长和发展。\n",
      "君不见黄河之水天上来，奔流到海不复回。\n",
      "君不见高堂明镜悲白发，朝如青丝暮成雪。\n",
      "人生得意须尽欢，莫使金樽空对月。\n",
      "天生我材必有用，千金散尽还复来。\n",
      "烹羊宰牛且为乐，会须一饮三百杯。\n",
      "岑夫子，丹丘生，将进酒，杯莫停。\n",
      "与君歌一曲，请君为我倾耳听。\n",
      "钟鼓馔玉不足贵，但愿长醉不复醒。\n",
      "古来圣贤皆寂寞，惟有饮者留其名。\n",
      "陈王昔时宴平乐，斗酒十千恣欢谑。\n",
      "主人何为言少钱，径须沽取对君酌。\n",
      "五花马，千金裘，呼儿将出换美酒，与尔同销万古愁。\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16\n",
    ")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_dir)\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"讲解一下“温故而知新”\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"背诵一下将进酒\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金子之所以会浮在水面上，是因为它的密度比水小。纯金的密度约为19.3克/立方厘米，而水的密度约为1克/立方厘米。因此，当金子进入水中时，它会漂浮在水的表面。这种现象被称为密度浮力。\n",
      "\n",
      "密度的定义是物体质量与体积的比值。在相同质量的情况下，密度较小的物体其体积较大，因此在水中时，密度较小的金子会漂浮在水的表面。\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"为什么金子浮在水面上\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
