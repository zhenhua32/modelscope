{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改预训练模型的缓存目录\n",
    "import os\n",
    "\n",
    "os.environ[\"MODELSCOPE_CACHE\"] = r\"G:\\code\\pretrain_model_dir\\_modelscope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-16 22:57:27,396 - modelscope - INFO - PyTorch version 1.13.1+cu117 Found.\n",
      "2023-10-16 22:57:27,399 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-10-16 22:57:27,399 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-10-16 22:57:27,648 - modelscope - INFO - Loading done! Current index file version is 1.8.3, with md5 f01211d2b2e03771df6500e25770738b and a total number of 895 components indexed\n",
      "2023-10-16 22:57:29,431 - modelscope - INFO - Use user-specified model revision: v1.0.5\n",
      "2023-10-16 22:57:31,708 - modelscope - WARNING - ('PIPELINES', 'text-generation', 'Baichuan-7B-text-generation-pipe') not found in ast index file\n",
      "2023-10-16 22:57:58,596 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-10-16 22:57:58,597 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-10-16 22:57:58,597 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\baichuan-inc\\\\baichuan-7B'}. trying to build by task and model information.\n",
      "2023-10-16 22:57:58,598 - modelscope - WARNING - No preprocessor key ('Baichuan-7B', 'text-generation') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
      "2023-10-16 22:57:58,601 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-10-16 22:57:58,601 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-10-16 22:57:58,601 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'G:\\\\code\\\\pretrain_model_dir\\\\_modelscope\\\\baichuan-inc\\\\baichuan-7B', 'first_sequence': 'sentence', 'device_map': 'auto'}. trying to build by task and model information.\n",
      "2023-10-16 22:57:58,602 - modelscope - WARNING - No preprocessor key ('Baichuan-7B', 'text-generation') found in PREPROCESSOR_MAP, skip building preprocessor.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.pipelines import pipeline\n",
    "text_generation_zh  = pipeline(task=Tasks.text_generation, model='baichuan-inc/baichuan-7B', device_map='auto',model_revision='v1.0.5')\n",
    "text_generation_zh._model_prepare = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '今天天气是真的好,阳光明媚,风和日丽,心情也跟着好了起来。 下午的时候,我正在家里看电视,忽然听见有人敲门,我打开门一看,原来是老爸回来了。 老爸一进门就说:“儿子,你妈呢?” 我说:“她出去买菜了。” 老爸说:“那你去帮我买包烟吧。” 我说:“好吧。” 于是我就去楼下的小卖部买烟。 到了小卖部,我对老板说:“老板,给我来包玉溪。” 老板说:“好嘞。” 我付完钱,正准备走的时候,老板忽然叫住我说:“小伙子,你还没给钱呢。” 我说:“什么钱?” 老板说:“你刚才买烟的钱。” 我说:“我买烟的时候,你不是说好嘞吗?怎么现在又说没给钱呢?” 老板笑着说:“我说的是好嘞,不是没给钱。” 我......'}\n"
     ]
    }
   ],
   "source": [
    "result_zh = text_generation_zh('今天天气是真的', min_length=10, max_length=512, num_beams=3,temperature=0.8,do_sample=False, early_stopping=True,top_k=50,top_p=0.8, repetition_penalty=1.2, length_penalty=1.2, no_repeat_ngram_size=6)\n",
    "print(result_zh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baichuan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 12:53:42,023 - modelscope - INFO - PyTorch version 2.1.0+cu121 Found.\n",
      "2023-12-09 12:53:42,026 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-12-09 12:53:42,026 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-12-09 12:53:42,156 - modelscope - INFO - Loading done! Current index file version is 1.9.4, with md5 bfb2e3fa67418cb37742cf2ad665ce75 and a total number of 945 components indexed\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-09 12:53:45,374 - modelscope - INFO - Use user-specified model revision: v1.0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\code\\pretrain_model_dir\\_modelscope\\baichuan-inc\\Baichuan2-13B-Chat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModelForCausalLM, AutoTokenizer,GenerationConfig\n",
    "model_dir = snapshot_download(\"baichuan-inc/Baichuan2-13B-Chat\", revision='v1.0.3')\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16\n",
    ")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_dir)\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"讲解一下“温故而知新”\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"背诵一下将进酒\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金子之所以会浮在水面上，是因为它的密度比水小。纯金的密度约为19.3克/立方厘米，而水的密度约为1克/立方厘米。因此，当金子进入水中时，它会漂浮在水的表面。这种现象被称为密度浮力。\n",
      "\n",
      "密度的定义是物体质量与体积的比值。在相同质量的情况下，密度较小的物体其体积较大，因此在水中时，密度较小的金子会漂浮在水的表面。\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"为什么金子浮在水面上\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试试量化加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 13:00:05,858 - modelscope - INFO - PyTorch version 2.1.0+cu121 Found.\n",
      "2023-12-09 13:00:05,862 - modelscope - INFO - TensorFlow version 2.8.0 Found.\n",
      "2023-12-09 13:00:05,862 - modelscope - INFO - Loading ast index from G:\\code\\pretrain_model_dir\\_modelscope\\ast_indexer\n",
      "2023-12-09 13:00:05,968 - modelscope - INFO - Loading done! Current index file version is 1.9.4, with md5 bfb2e3fa67418cb37742cf2ad665ce75 and a total number of 945 components indexed\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-09 13:00:10,519 - modelscope - INFO - Use user-specified model revision: v1.0.3\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.04s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    False,\n",
    "    True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model_dir = snapshot_download(\"baichuan-inc/Baichuan2-13B-Chat\", revision=\"v1.0.3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"温故而知新\"是一句源自《论语·为政》的古文名言，它意味着通过回顾过去，我们可以更好地理解现在和未来。这句话的核心思想是：我们要不断地学习和反思，以便从过去的经验中汲取智慧，从而更好地应对现在的挑战和未来的变化。\n",
      "\n",
      "\"温故\"指的是回顾过去，研究历史，了解我们的根源和发展过程。通过对过去的深入了解，我们可以更好地理解现在的状况，发现其中的规律和挑战。\n",
      "\n",
      "\"知新\"则是指通过学习、实践和创新，发现新的知识和技能，以适应不断变化的世界。我们要保持开放的心态，勇于尝试和探索，以便在不断进步的社会中找到自己的位置和价值。\n",
      "\n",
      "总之，\"温故而知新\"是一种积极的人生态度，鼓励我们既要珍视传统，又要拥抱变革，从而实现个人和社会的和谐发展。\n",
      "《将进酒》是唐代诗人李白创作的一首诗，全文如下：\n",
      "\n",
      "君不见，黄河之水天上来，奔流到海不复回。\n",
      "君不见，高堂明镜悲白发，朝如青丝暮成雪。\n",
      "人生得意须尽欢，莫使金樽空对月。\n",
      "天生我材必有用，千金散尽还复来。\n",
      "烹羊宰牛且为乐，会须一饮三百杯。\n",
      "岑夫子，丹丘生，将进酒，杯莫停。\n",
      "与君歌一曲，请君为我倾耳听。\n",
      "钟鼓馔玉不足贵，但愿长醉不复醒。\n",
      "古来圣贤皆寂寞，惟有饮者留其名。\n",
      "陈王昔时宴平乐，斗酒十千恣欢谑。\n",
      "主人何为言少钱，径须沽取对君酌。\n",
      "五花马，千金裘，呼儿将出换美酒，与尔同销万古愁。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"讲解一下“温故而知新”\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n",
    "messages.append({'role': 'assistant', 'content': response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"背诵一下将进酒\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金子浮在水面上的原因主要是由于其密度较小。纯金的密度大约在19.32克/立方厘米左右，而水的密度大约是1克/立方厘米。因此，金子在水中会受到浮力的作用，使其漂浮在水面上。这是因为金子的重量小于它所占有的空间所需的压力。这就是为什么金子会浮在水面上的原因。\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"为什么金子浮在水面上\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
